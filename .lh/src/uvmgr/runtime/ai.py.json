{
    "sourceFile": "src/uvmgr/runtime/ai.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1748235448437,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1748236112335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,56 +1,90 @@\n \"\"\"\n-Thin wrapper around DSPy ChatModel plus provider registry.\n+uvmgr.runtime.ai\n+----------------\n+Single source of truth for creating DSPy `LM` objects that work with:\n+\n+* **openai/** models  → remote OpenAI\n+* **ollama/** models → local Ollama server (OpenAI-compatible API)\n+\n+The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n+*actually* call the LM; higher layers must never touch DSPy directly.\n \"\"\"\n \n from __future__ import annotations\n \n import logging\n+import os\n+from typing import List\n+\n+import dspy\n from uvmgr.core.telemetry import span\n-from uvmgr.core.config import env_or\n \n _log = logging.getLogger(\"uvmgr.runtime.ai\")\n \n-try:\n-    import dspy\n-except ImportError:  # optional dep\n-    raise RuntimeError(\"pip install dspy to use uvmgr ai commands\") from None\n+# --------------------------------------------------------------------------- #\n+# LM factory                                                                  #\n+# --------------------------------------------------------------------------- #\n \n \n-def _get_lm(model: str) -> dspy.ChatModel:\n-    provider = env_or(\"UVMGR_LM_PROVIDER\", \"openai\")\n-    if provider == \"openai\":\n-        api_key = env_or(\"OPENAI_API_KEY\")\n-        if not api_key:\n-            raise RuntimeError(\"OPENAI_API_KEY missing\")\n-        return dspy.OpenAI(model=model, api_key=api_key)\n-    raise ValueError(f\"unknown provider {provider}\")\n+def _init_lm(model: str, **kw) -> dspy.LM:\n+    \"\"\"\n+    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n+    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n+    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n+    \"\"\"\n+    cfg: dict = {\n+        \"model\": model,\n+        \"temperature\": 0.0,\n+        \"max_tokens\": 2048,\n+        **kw,\n+    }\n \n+    if model.startswith(\"ollama/\"):\n+        # strip prefix; Ollama doesn't need it\n+        cfg[\"model\"] = model.removeprefix(\"ollama/\")\n+        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n+    elif model.startswith(\"openai/\"):\n+        cfg[\"model\"] = model.removeprefix(\"openai/\")\n+        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n+        if not cfg[\"api_key\"]:\n+            raise RuntimeError(\"OPENAI_API_KEY not set\")\n \n+    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n+    dspy.settings.configure(lm=lm, experimental=True)\n+    return lm\n+\n+\n+# --------------------------------------------------------------------------- #\n+# Public helpers                                                              #\n+# --------------------------------------------------------------------------- #\n+\n+\n def ask(model: str, prompt: str) -> str:\n     with span(\"ai.ask\", model=model):\n-        lm = _get_lm(model)\n+        lm = _init_lm(model)\n         return lm(prompt)\n \n \n-def outline(model: str, topic: str, n: int = 5) -> list[str]:\n-    p = f\"Generate an ordered list of {n} bullet points about {topic}\"\n-    text = ask(model, p)\n-    return [line.strip(\"•- \").rstrip() for line in text.splitlines() if line.strip()]\n+def outline(model: str, topic: str, n: int = 5) -> List[str]:\n+    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n+    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n \n \n def fix_tests(model: str) -> str:\n-    from uvmgr.core.process import run_logged, run\n-    from uvmgr.core.shell import colour\n+    \"\"\"\n+    Run failing tests once, send the traceback to the LM, ask for a *unified\n+    diff* patch that fixes the bug.  Return the diff (caller decides what to\n+    do with it).\n+    \"\"\"\n+    from uvmgr.core.process import run\n+    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n+    if \"failed\" not in failure:\n+        return \"\"\n \n-    run_logged(\"pytest -q\", capture=True)\n-    failed = run(\"pytest --maxfail=1 -q\", capture=True)\n-    if \"failed\" not in failed:\n-        colour(\"✔ tests already pass\", \"green\")\n-        return \"\"\n-    patch = ask(\n-        model,\n-        \"You are an expert developer. Given the following pytest failure, \"\n-        \"print a unified diff patch that fixes the bug.\\n\\n\"\n-        f\"{failed}\",\n+    prompt = (\n+        \"You are an expert Python developer. Analyse the following pytest \"\n+        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n+        f\"\\n\\n{failure}\"\n     )\n-    return patch\n+    return ask(model, prompt)\n"
                },
                {
                    "date": 1748244758919,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,90 @@\n+\"\"\"\n+uvmgr.runtime.ai\n+----------------\n+Single source of truth for creating DSPy `LM` objects that work with:\n+\n+* **openai/** models  → remote OpenAI\n+* **ollama/** models → local Ollama server (OpenAI-compatible API)\n+\n+The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n+*actually* call the LM; higher layers must never touch DSPy directly.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import os\n+from typing import List\n+\n+import dspy\n+from uvmgr.core.telemetry import span\n+\n+_log = logging.getLogger(\"uvmgr.runtime.ai\")\n+\n+# --------------------------------------------------------------------------- #\n+# LM factory                                                                  #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def _init_lm(model: str, **kw) -> dspy.LM:\n+    \"\"\"\n+    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n+    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n+    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n+    \"\"\"\n+    cfg: dict = {\n+        \"model\": model,\n+        \"temperature\": 0.0,\n+        \"max_tokens\": 2048,\n+        **kw,\n+    }\n+\n+    if model.startswith(\"ollama/\"):\n+        # strip prefix; Ollama doesn't need it\n+        cfg[\"modl\"] = model.removeprefix(\"ollama/\")\n+        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n+    elif model.startswith(\"openai/\"):\n+        cfg[\"model\"] = model.removeprefix(\"openai/\")\n+        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n+        if not cfg[\"api_key\"]:\n+            raise RuntimeError(\"OPENAI_API_KEY not set\")\n+\n+    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n+    dspy.settings.configure(lm=lm, experimental=True)\n+    return lm\n+\n+\n+# --------------------------------------------------------------------------- #\n+# Public helpers                                                              #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def ask(model: str, prompt: str) -> str:\n+    with span(\"ai.ask\", model=model):\n+        lm = _init_lm(model)\n+        return lm(prompt)\n+\n+\n+def outline(model: str, topic: str, n: int = 5) -> List[str]:\n+    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n+    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n+\n+\n+def fix_tests(model: str) -> str:\n+    \"\"\"\n+    Run failing tests once, send the traceback to the LM, ask for a *unified\n+    diff* patch that fixes the bug.  Return the diff (caller decides what to\n+    do with it).\n+    \"\"\"\n+    from uvmgr.core.process import run\n+    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n+    if \"failed\" not in failure:\n+        return \"\"\n+\n+    prompt = (\n+        \"You are an expert Python developer. Analyse the following pytest \"\n+        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n+        f\"\\n\\n{failure}\"\n+    )\n+    return ask(model, prompt)\n"
                },
                {
                    "date": 1748244772563,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,89 @@\n+\"\"\"\n+uvmgr.runtime.ai\n+----------------\n+Single source of truth for creating DSPy `LM` objects that work with:\n+\n+* **openai/** models  → remote OpenAI\n+* **ollama/** models → local Ollama server (OpenAI-compatible API)\n+\n+The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n+*actually* call the LM; higher layers must never touch DSPy directly.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import os\n+from typing import List\n+\n+import dspy\n+from uvmgr.core.telemetry import span\n+\n+_log = logging.getLogger(\"uvmgr.runtime.ai\")\n+\n+# --------------------------------------------------------------------------- #\n+# LM factory                                                                  #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def _init_lm(model: str, **kw) -> dspy.LM:\n+    \"\"\"\n+    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n+    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n+    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n+    \"\"\"\n+    cfg: dict = {\n+        \"model\": model,\n+        \"temperature\": 0.0,\n+        \"max_tokens\": 2048,\n+        **kw,\n+    }\n+\n+    if model.startswith(\"ollama/\"):\n+        # strip prefix; Ollama doesn't need it\n+        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n+    elif model.startswith(\"openai/\"):\n+        cfg[\"model\"] = model.removeprefix(\"openai/\")\n+        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n+        if not cfg[\"api_key\"]:\n+            raise RuntimeError(\"OPENAI_API_KEY not set\")\n+\n+    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n+    dspy.settings.configure(lm=lm, experimental=True)\n+    return lm\n+\n+\n+# --------------------------------------------------------------------------- #\n+# Public helpers                                                              #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def ask(model: str, prompt: str) -> str:\n+    with span(\"ai.ask\", model=model):\n+        lm = _init_lm(model)\n+        return lm(prompt)\n+\n+\n+def outline(model: str, topic: str, n: int = 5) -> List[str]:\n+    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n+    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n+\n+\n+def fix_tests(model: str) -> str:\n+    \"\"\"\n+    Run failing tests once, send the traceback to the LM, ask for a *unified\n+    diff* patch that fixes the bug.  Return the diff (caller decides what to\n+    do with it).\n+    \"\"\"\n+    from uvmgr.core.process import run\n+    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n+    if \"failed\" not in failure:\n+        return \"\"\n+\n+    prompt = (\n+        \"You are an expert Python developer. Analyse the following pytest \"\n+        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n+        f\"\\n\\n{failure}\"\n+    )\n+    return ask(model, prompt)\n"
                },
                {
                    "date": 1748244813279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,88 @@\n+\"\"\"\n+uvmgr.runtime.ai\n+----------------\n+Single source of truth for creating DSPy `LM` objects that work with:\n+\n+* **openai/** models  → remote OpenAI\n+* **ollama/** models → local Ollama server (OpenAI-compatible API)\n+\n+The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n+*actually* call the LM; higher layers must never touch DSPy directly.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import os\n+from typing import List\n+\n+import dspy\n+from uvmgr.core.telemetry import span\n+\n+_log = logging.getLogger(\"uvmgr.runtime.ai\")\n+\n+# --------------------------------------------------------------------------- #\n+# LM factory                                                                  #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def _init_lm(model: str = \"openai/gpt-4.1-nano\", **kw) -> dspy.LM:\n+    \"\"\"\n+    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n+    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n+    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n+    \"\"\"\n+    cfg: dict = {\n+        \"model\": model,\n+        \"temperature\": 0.0,\n+        \"max_tokens\": 2048,\n+        **kw,\n+    }\n+\n+    if model.startswith(\"ollama/\"):\n+        # strip prefix; Ollama doesn't need it\n+        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n+    elif model.startswith(\"openai/\"):\n+        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n+        if not cfg[\"api_key\"]:\n+            raise RuntimeError(\"OPENAI_API_KEY not set\")\n+\n+    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n+    dspy.settings.configure(lm=lm, experimental=True)\n+    return lm\n+\n+\n+# --------------------------------------------------------------------------- #\n+# Public helpers                                                              #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def ask(model: str, prompt: str) -> str:\n+    with span(\"ai.ask\", model=model):\n+        lm = _init_lm(model)\n+        return lm(prompt)\n+\n+\n+def outline(model: str, topic: str, n: int = 5) -> List[str]:\n+    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n+    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n+\n+\n+def fix_tests(model: str) -> str:\n+    \"\"\"\n+    Run failing tests once, send the traceback to the LM, ask for a *unified\n+    diff* patch that fixes the bug.  Return the diff (caller decides what to\n+    do with it).\n+    \"\"\"\n+    from uvmgr.core.process import run\n+    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n+    if \"failed\" not in failure:\n+        return \"\"\n+\n+    prompt = (\n+        \"You are an expert Python developer. Analyse the following pytest \"\n+        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n+        f\"\\n\\n{failure}\"\n+    )\n+    return ask(model, prompt)\n"
                },
                {
                    "date": 1748244827082,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,88 @@\n+\"\"\"\n+uvmgr.runtime.ai\n+----------------\n+Single source of truth for creating DSPy `LM` objects that work with:\n+\n+* **openai/** models  → remote OpenAI\n+* **ollama/** models → local Ollama server (OpenAI-compatible API)\n+\n+The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n+*actually* call the LM; higher layers must never touch DSPy directly.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import logging\n+import os\n+from typing import List\n+\n+import dspy\n+from uvmgr.core.telemetry import span\n+\n+_log = logging.getLogger(\"uvmgr.runtime.ai\")\n+\n+# --------------------------------------------------------------------------- #\n+# LM factory                                                                  #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def _init_lm(model: str, **kw) -> dspy.LM:\n+    \"\"\"\n+    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n+    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n+    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n+    \"\"\"\n+    cfg: dict = {\n+        \"model\": model,\n+        \"temperature\": 0.0,\n+        \"max_tokens\": 2048,\n+        **kw,\n+    }\n+\n+    if model.startswith(\"ollama/\"):\n+        # strip prefix; Ollama doesn't need it\n+        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n+    elif model.startswith(\"openai/\"):\n+        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n+        if not cfg[\"api_key\"]:\n+            raise RuntimeError(\"OPENAI_API_KEY not set\")\n+\n+    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n+    dspy.settings.configure(lm=lm, experimental=True)\n+    return lm\n+\n+\n+# --------------------------------------------------------------------------- #\n+# Public helpers                                                              #\n+# --------------------------------------------------------------------------- #\n+\n+\n+def ask(model: str, prompt: str) -> str:\n+    with span(\"ai.ask\", model=model):\n+        lm = _init_lm(model)\n+        return lm(prompt)\n+\n+\n+def outline(model: str, topic: str, n: int = 5) -> List[str]:\n+    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n+    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n+\n+\n+def fix_tests(model: str) -> str:\n+    \"\"\"\n+    Run failing tests once, send the traceback to the LM, ask for a *unified\n+    diff* patch that fixes the bug.  Return the diff (caller decides what to\n+    do with it).\n+    \"\"\"\n+    from uvmgr.core.process import run\n+    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n+    if \"failed\" not in failure:\n+        return \"\"\n+\n+    prompt = (\n+        \"You are an expert Python developer. Analyse the following pytest \"\n+        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n+        f\"\\n\\n{failure}\"\n+    )\n+    return ask(model, prompt)\n"
                },
                {
                    "date": 1748245028869,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     }\n \n     if model.startswith(\"ollama/\"):\n         # strip prefix; Ollama doesn't need it\n-        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n+        # cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n         cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n     elif model.startswith(\"openai/\"):\n         cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n         if not cfg[\"api_key\"]:\n@@ -85,361 +85,4 @@\n         \"failure output and propose a **unified diff patch** that fixes the bug.\"\n         f\"\\n\\n{failure}\"\n     )\n     return ask(model, prompt)\n-\"\"\"\n-uvmgr.runtime.ai\n-----------------\n-Single source of truth for creating DSPy `LM` objects that work with:\n-\n-* **openai/** models  → remote OpenAI\n-* **ollama/** models → local Ollama server (OpenAI-compatible API)\n-\n-The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n-*actually* call the LM; higher layers must never touch DSPy directly.\n-\"\"\"\n-\n-from __future__ import annotations\n-\n-import logging\n-import os\n-from typing import List\n-\n-import dspy\n-from uvmgr.core.telemetry import span\n-\n-_log = logging.getLogger(\"uvmgr.runtime.ai\")\n-\n-# --------------------------------------------------------------------------- #\n-# LM factory                                                                  #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def _init_lm(model: str = \"openai/gpt-4.1-nano\", **kw) -> dspy.LM:\n-    \"\"\"\n-    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n-    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n-    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n-    \"\"\"\n-    cfg: dict = {\n-        \"model\": model,\n-        \"temperature\": 0.0,\n-        \"max_tokens\": 2048,\n-        **kw,\n-    }\n-\n-    if model.startswith(\"ollama/\"):\n-        # strip prefix; Ollama doesn't need it\n-        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n-        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n-    elif model.startswith(\"openai/\"):\n-        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n-        if not cfg[\"api_key\"]:\n-            raise RuntimeError(\"OPENAI_API_KEY not set\")\n-\n-    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n-    dspy.settings.configure(lm=lm, experimental=True)\n-    return lm\n-\n-\n-# --------------------------------------------------------------------------- #\n-# Public helpers                                                              #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def ask(model: str, prompt: str) -> str:\n-    with span(\"ai.ask\", model=model):\n-        lm = _init_lm(model)\n-        return lm(prompt)\n-\n-\n-def outline(model: str, topic: str, n: int = 5) -> List[str]:\n-    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n-    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n-\n-\n-def fix_tests(model: str) -> str:\n-    \"\"\"\n-    Run failing tests once, send the traceback to the LM, ask for a *unified\n-    diff* patch that fixes the bug.  Return the diff (caller decides what to\n-    do with it).\n-    \"\"\"\n-    from uvmgr.core.process import run\n-    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n-    if \"failed\" not in failure:\n-        return \"\"\n-\n-    prompt = (\n-        \"You are an expert Python developer. Analyse the following pytest \"\n-        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n-        f\"\\n\\n{failure}\"\n-    )\n-    return ask(model, prompt)\n-\"\"\"\n-uvmgr.runtime.ai\n-----------------\n-Single source of truth for creating DSPy `LM` objects that work with:\n-\n-* **openai/** models  → remote OpenAI\n-* **ollama/** models → local Ollama server (OpenAI-compatible API)\n-\n-The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n-*actually* call the LM; higher layers must never touch DSPy directly.\n-\"\"\"\n-\n-from __future__ import annotations\n-\n-import logging\n-import os\n-from typing import List\n-\n-import dspy\n-from uvmgr.core.telemetry import span\n-\n-_log = logging.getLogger(\"uvmgr.runtime.ai\")\n-\n-# --------------------------------------------------------------------------- #\n-# LM factory                                                                  #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def _init_lm(model: str, **kw) -> dspy.LM:\n-    \"\"\"\n-    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n-    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n-    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n-    \"\"\"\n-    cfg: dict = {\n-        \"model\": model,\n-        \"temperature\": 0.0,\n-        \"max_tokens\": 2048,\n-        **kw,\n-    }\n-\n-    if model.startswith(\"ollama/\"):\n-        # strip prefix; Ollama doesn't need it\n-        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n-        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n-    elif model.startswith(\"openai/\"):\n-        cfg[\"model\"] = model.removeprefix(\"openai/\")\n-        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n-        if not cfg[\"api_key\"]:\n-            raise RuntimeError(\"OPENAI_API_KEY not set\")\n-\n-    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n-    dspy.settings.configure(lm=lm, experimental=True)\n-    return lm\n-\n-\n-# --------------------------------------------------------------------------- #\n-# Public helpers                                                              #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def ask(model: str, prompt: str) -> str:\n-    with span(\"ai.ask\", model=model):\n-        lm = _init_lm(model)\n-        return lm(prompt)\n-\n-\n-def outline(model: str, topic: str, n: int = 5) -> List[str]:\n-    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n-    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n-\n-\n-def fix_tests(model: str) -> str:\n-    \"\"\"\n-    Run failing tests once, send the traceback to the LM, ask for a *unified\n-    diff* patch that fixes the bug.  Return the diff (caller decides what to\n-    do with it).\n-    \"\"\"\n-    from uvmgr.core.process import run\n-    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n-    if \"failed\" not in failure:\n-        return \"\"\n-\n-    prompt = (\n-        \"You are an expert Python developer. Analyse the following pytest \"\n-        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n-        f\"\\n\\n{failure}\"\n-    )\n-    return ask(model, prompt)\n-\"\"\"\n-uvmgr.runtime.ai\n-----------------\n-Single source of truth for creating DSPy `LM` objects that work with:\n-\n-* **openai/** models  → remote OpenAI\n-* **ollama/** models → local Ollama server (OpenAI-compatible API)\n-\n-The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n-*actually* call the LM; higher layers must never touch DSPy directly.\n-\"\"\"\n-\n-from __future__ import annotations\n-\n-import logging\n-import os\n-from typing import List\n-\n-import dspy\n-from uvmgr.core.telemetry import span\n-\n-_log = logging.getLogger(\"uvmgr.runtime.ai\")\n-\n-# --------------------------------------------------------------------------- #\n-# LM factory                                                                  #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def _init_lm(model: str, **kw) -> dspy.LM:\n-    \"\"\"\n-    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n-    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n-    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n-    \"\"\"\n-    cfg: dict = {\n-        \"model\": model,\n-        \"temperature\": 0.0,\n-        \"max_tokens\": 2048,\n-        **kw,\n-    }\n-\n-    if model.startswith(\"ollama/\"):\n-        # strip prefix; Ollama doesn't need it\n-        cfg[\"modl\"] = model.removeprefix(\"ollama/\")\n-        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n-        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n-    elif model.startswith(\"openai/\"):\n-        cfg[\"model\"] = model.removeprefix(\"openai/\")\n-        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n-        if not cfg[\"api_key\"]:\n-            raise RuntimeError(\"OPENAI_API_KEY not set\")\n-\n-    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n-    dspy.settings.configure(lm=lm, experimental=True)\n-    return lm\n-\n-\n-# --------------------------------------------------------------------------- #\n-# Public helpers                                                              #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def ask(model: str, prompt: str) -> str:\n-    with span(\"ai.ask\", model=model):\n-        lm = _init_lm(model)\n-        return lm(prompt)\n-\n-\n-def outline(model: str, topic: str, n: int = 5) -> List[str]:\n-    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n-    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n-\n-\n-def fix_tests(model: str) -> str:\n-    \"\"\"\n-    Run failing tests once, send the traceback to the LM, ask for a *unified\n-    diff* patch that fixes the bug.  Return the diff (caller decides what to\n-    do with it).\n-    \"\"\"\n-    from uvmgr.core.process import run\n-    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n-    if \"failed\" not in failure:\n-        return \"\"\n-\n-    prompt = (\n-        \"You are an expert Python developer. Analyse the following pytest \"\n-        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n-        f\"\\n\\n{failure}\"\n-    )\n-    return ask(model, prompt)\n-\"\"\"\n-uvmgr.runtime.ai\n-----------------\n-Single source of truth for creating DSPy `LM` objects that work with:\n-\n-* **openai/** models  → remote OpenAI\n-* **ollama/** models → local Ollama server (OpenAI-compatible API)\n-\n-The public helpers (`ask`, `outline`, `fix_tests`) are thin wrappers that\n-*actually* call the LM; higher layers must never touch DSPy directly.\n-\"\"\"\n-\n-from __future__ import annotations\n-\n-import logging\n-import os\n-from typing import List\n-\n-import dspy\n-from uvmgr.core.telemetry import span\n-\n-_log = logging.getLogger(\"uvmgr.runtime.ai\")\n-\n-# --------------------------------------------------------------------------- #\n-# LM factory                                                                  #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def _init_lm(model: str, **kw) -> dspy.LM:\n-    \"\"\"\n-    Creates a DSPy LM with sane defaults.  If the *model* string begins with\n-    ``ollama/``, we automatically point the OpenAI-compatible base URL at the\n-    local Ollama server (or whatever `OLLAMA_BASE` env var says).\n-    \"\"\"\n-    cfg: dict = {\n-        \"model\": model,\n-        \"temperature\": 0.0,\n-        \"max_tokens\": 2048,\n-        **kw,\n-    }\n-\n-    if model.startswith(\"ollama/\"):\n-        # strip prefix; Ollama doesn't need it\n-        cfg[\"model\"] = model.removeprefix(\"ollama/\")\n-        cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n-        cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n-    elif model.startswith(\"openai/\"):\n-        cfg[\"model\"] = model.removeprefix(\"openai/\")\n-        cfg[\"api_key\"] = os.getenv(\"OPENAI_API_KEY\")\n-        if not cfg[\"api_key\"]:\n-            raise RuntimeError(\"OPENAI_API_KEY not set\")\n-\n-    lm = dspy.LM(**{k: v for k, v in cfg.items() if v is not None})\n-    dspy.settings.configure(lm=lm, experimental=True)\n-    return lm\n-\n-\n-# --------------------------------------------------------------------------- #\n-# Public helpers                                                              #\n-# --------------------------------------------------------------------------- #\n-\n-\n-def ask(model: str, prompt: str) -> str:\n-    with span(\"ai.ask\", model=model):\n-        lm = _init_lm(model)\n-        return lm(prompt)\n-\n-\n-def outline(model: str, topic: str, n: int = 5) -> List[str]:\n-    bullets = ask(model, f\"List {n} key points about {topic}:\\n•\").splitlines()\n-    return [b.lstrip(\"•- \").strip() for b in bullets if b.strip()]\n-\n-\n-def fix_tests(model: str) -> str:\n-    \"\"\"\n-    Run failing tests once, send the traceback to the LM, ask for a *unified\n-    diff* patch that fixes the bug.  Return the diff (caller decides what to\n-    do with it).\n-    \"\"\"\n-    from uvmgr.core.process import run\n-    failure = run(\"pytest --maxfail=1 -q\", capture=True)\n-    if \"failed\" not in failure:\n-        return \"\"\n-\n-    prompt = (\n-        \"You are an expert Python developer. Analyse the following pytest \"\n-        \"failure output and propose a **unified diff patch** that fixes the bug.\"\n-        f\"\\n\\n{failure}\"\n-    )\n-    return ask(model, prompt)\n"
                },
                {
                    "date": 1748245036282,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n         \"max_tokens\": 2048,\n         **kw,\n     }\n \n-    if model.startswith(\"ollama/\"):\n+    # if model.startswith(\"ollama/\"):\n         # strip prefix; Ollama doesn't need it\n         # cfg[\"api_base\"] = os.getenv(\"OLLAMA_BASE\", \"http://localhost:11434/v1\")\n         # cfg[\"api_key\"] = os.getenv(\"OLLAMA_API_KEY\", \"ollama-no-key\")  # dummy token\n     elif model.startswith(\"openai/\"):\n"
                }
            ],
            "date": 1748235448437,
            "name": "Commit-0",
            "content": "\"\"\"\nThin wrapper around DSPy ChatModel plus provider registry.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom uvmgr.core.telemetry import span\nfrom uvmgr.core.config import env_or\n\n_log = logging.getLogger(\"uvmgr.runtime.ai\")\n\ntry:\n    import dspy\nexcept ImportError:  # optional dep\n    raise RuntimeError(\"pip install dspy to use uvmgr ai commands\") from None\n\n\ndef _get_lm(model: str) -> dspy.ChatModel:\n    provider = env_or(\"UVMGR_LM_PROVIDER\", \"openai\")\n    if provider == \"openai\":\n        api_key = env_or(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise RuntimeError(\"OPENAI_API_KEY missing\")\n        return dspy.OpenAI(model=model, api_key=api_key)\n    raise ValueError(f\"unknown provider {provider}\")\n\n\ndef ask(model: str, prompt: str) -> str:\n    with span(\"ai.ask\", model=model):\n        lm = _get_lm(model)\n        return lm(prompt)\n\n\ndef outline(model: str, topic: str, n: int = 5) -> list[str]:\n    p = f\"Generate an ordered list of {n} bullet points about {topic}\"\n    text = ask(model, p)\n    return [line.strip(\"•- \").rstrip() for line in text.splitlines() if line.strip()]\n\n\ndef fix_tests(model: str) -> str:\n    from uvmgr.core.process import run_logged, run\n    from uvmgr.core.shell import colour\n\n    run_logged(\"pytest -q\", capture=True)\n    failed = run(\"pytest --maxfail=1 -q\", capture=True)\n    if \"failed\" not in failed:\n        colour(\"✔ tests already pass\", \"green\")\n        return \"\"\n    patch = ask(\n        model,\n        \"You are an expert developer. Given the following pytest failure, \"\n        \"print a unified diff patch that fixes the bug.\\n\\n\"\n        f\"{failed}\",\n    )\n    return patch\n"
        }
    ]
}